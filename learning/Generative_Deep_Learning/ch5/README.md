## Chapter 5. 자기회귀 모델(Autoregressive Model)

---

**LSTM(Long Short-Term Memory network)는** 순환 신경망(RNN, Recurrent Neural Nework)의 한 종류입니다.
기존 RNN의 문제인 기울기 소멸 문제를 방지하도록 개발되었다. 

RNN은 입력과 출력을 시쿼스 단위로 처리하는 시퀀스 모델입니다. 인간은 생각을 처음부터 시작하지 않습니다.
무언가 글을 읽거나 생각을 할 때도 지금까지의 경험을 토대로 이를 해석합니다. 일반적인 nerual network는 이처럼 동작하지 않습니다.

Recurrent neural network(RNN)은 이러한 일반적인 nerual network의 문제점을 해결하고자 만든 모델입니다.
RNN은 아래 그림처럼 유닛 간의 연결이 순환적 구조를 가지는 특징이 있습니다. 

<img src="https://github.com/user-attachments/assets/dd37c716-369a-4a48-af6f-844a424de460">

RNN이 처음 소개되었을 때, 층 구조가 단순했고 tanh 함수 하나로 구성되었습니다.
그러나 이러한 방식은 **기울기 소멸 문제(Gradient vanishing)가** 나타나 시퀀스가 너무 긴 데이터에서는 사용하기 힘듭니다.

LSTM은 순환 구조 여러개의 LSTM 셀로 구성되어 있습니다. 
이 LSTM 셀은 이전 은닉 상태 h(t-1)과 현재 단어 임베딩 x(t)를 입력 받아 새로운 입력 상태 h(t)를 출력합니다.
하나의 LSTM은 하나의 셀 상태(Cell-state)인 C(t)를 관리합니다. 

<img src="https://github.com/user-attachments/assets/c62f886c-b246-46ef-b7b8-55ec4b3f0503" height="300">

하나의 LSTM 셀은 다음과 같이 업데이트됩니다.

- 이전 은닉층의 출력 h(t-1)는 현재 단어 임베딩 x(t)와 연결되어 시그모이드 활성화 함수가 있는 삭제 게이트로 전달됩니다. 
이 값은 이전 셀의 상태 C(t-1)을 얼마나 유지할지 결정합니다. 이 결과 벡터를 f(t)라고 정의하겠습니다.
- 위의 삭제 게이트에 전달된 내용은 마찬가지로 시그모이드 활성화 함수가 있는 입력 게이트로도 전달됩니다. 
이 값은 이전 셀 상태 C(t-1)에 얼마나 새로운 정보를 추가할지를 결정합니다. 이 결과 벡터를 i(t)라고 정의하겠습니다.
- 이 연결된 벡터는 tanh 활성화 함수가 있는 또 다른 연결 층으로 전달되어 벡터 C'(t)를 만듭니다.
이 벡터는 셀이 저장하려는 새로운 벡터는 담습니다. 
- f(t)와 C(t-1)을 원소별 곱셈한 값과 i(t)와 C'(t)의 원소별 곱셈을 더합니다. 
이전 상태 C(t-1)에서 일부분을 삭제하고 새로운 정보를 더해 현재 셀의 상태 C(t)를 구하게 됩니다. 
- 이 연결 벡터는 시그모이드 활성화 함수가 있는 출력 게이트로도 전달됩니다.
이 값은 업데이트 된 C(t)를 셀의 출력으로 얼마나 내보낼지를 결정합니다. 이 벡트를 o(t)라고 정의합니다.
- C(t)는  tanh 활성화 함수가 적용된 후, o(t)와 곱셈하여 새로운 은닉층 h(t)를 생성합니다.
