{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-19T10:52:15.704724Z",
     "start_time": "2024-07-19T10:52:13.199608Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 19:52:13.678160: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-19 19:52:13.919425: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-19 19:52:13.919450: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-19 19:52:13.920923: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-19 19:52:14.035981: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-19 19:52:14.849731: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T10:56:47.659828Z",
     "start_time": "2024-07-19T10:56:47.328671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터셋은 http://oreil.ly/laNUt 에서 직접 다운로드 받았습니다.\n",
    "with open('./full_format_recipes.json') as json_data:\n",
    "    recipe_data = json.load(json_data)\n",
    "    \n",
    "filtered_data = [\n",
    "    'Recipe for ' + x['title'] + \" | \" + ''.join(x['directions']) for x in recipe_data\n",
    "    if 'title' in x and x['title'] is not None and 'directions' in x and x['directions'] is not None\n",
    "]"
   ],
   "id": "8b6077c6dc427f1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T10:57:43.081389Z",
     "start_time": "2024-07-19T10:57:43.078738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_recipes = len(filtered_data)\n",
    "print('{}개 레시피 로드'.format(n_recipes))\n",
    "\n",
    "example = filtered_data[19]\n",
    "print(example)"
   ],
   "id": "e9076eb399608ce6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20111개 레시피 로드\n",
      "Recipe for Tuna, Asparagus, and New Potato Salad with Chive Vinaigrette and Fried Capers  | Puree first 5 ingredients in blender until smooth. With machine running, gradually add vegetable oil, then olive oil. Season to taste with salt and pepper. DO AHEAD: Can be made 1 day ahead. Cover and chill.Cook asparagus in large skillet of boiling salted water until just tender, 4 to 5 minutes. Transfer asparagus to 13x9x2-inch pan of ice water to cool. Drain asparagus and pat dry. DO AHEAD: Can be made 8 hours ahead. Wrap in paper towels, then plastic, and chill.Place potatoes in large saucepan. Add enough water to cover potatoes by 1 inch. Sprinkle with salt. Bring to boil and cook until potatoes are tender, 10 to 15 minutes, depending on size of potatoes. Drain; let cool 5 minutes. Place in medium bowl. Add 1/4 cup vinaigrette; toss to coat. Season to taste with salt and pepper.Heat olive oil in small skillet over medium-high heat. Add capers and fry until capers are crisp and open like flowers, stirring often, 45 to 60 seconds. Using slotted spoon, transfer capers to paper towels to drain. DO AHEAD: Potatoes and capers can be made 2 hours ahead. Let stand at room temperature.Place asparagus in large bowl. Add 2 tablespoons vinaigrette and toss to coat. Toss potatoes again to coat, adding 1 more tablespoon vinaigrette if dry. Place greens and radishes in another large bowl. Toss with enough vinaigrette to coat. Spread greens and radishes over large platter. Arrange potatoes, asparagus, eggs, and tuna atop greens. Drizzle some vinaigrette over tuna. Sprinkle with fried capers and chive blossoms, if desired.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 토큰화(tokenization)\n",
    "\n",
    "토큰화는 텍스트를 단어나 문자와 같은 개별 단위로 나누는 작업.  \n",
    "텍스트 토큰화는 다양한 방법이 존재하고 방법에 따라 이후 모델의 출력에 많은 영향을 미칩니다."
   ],
   "id": "c1f84761903be81e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:10:35.553908Z",
     "start_time": "2024-07-19T11:10:32.098623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_punctuation(s):\n",
    "    s = re.sub(f'([{string.punctuation}])', r' \\1', s)\n",
    "    s = re.sub(' +', ' ', s)\n",
    "    return s\n",
    "\n",
    "text_data = [pad_punctuation(x) for x in filtered_data]\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(text_data).batch(32).shuffle(1000)\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize = 'lower',\n",
    "    max_tokens = 10000,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = 200 + 1,\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ],
   "id": "1d41d3718104dcaf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 20:10:33.163827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 20:10:33.270546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 20:10:33.270578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 20:10:33.272059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 20:10:33.272083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 20:10:33.272092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 20:10:33.402983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 20:10:33.403024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 20:10:33.403030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-19 20:10:33.403051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 20:10:33.403066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21770 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:11:53.511259Z",
     "start_time": "2024-07-19T11:11:53.508506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, word in enumerate(vocab[:10]):\n",
    "    print('{}: {}'.format(i, word))"
   ],
   "id": "48884122cca8fb7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "1: [UNK]\n",
      "2: ,\n",
      "3: .\n",
      "4: and\n",
      "5: to\n",
      "6: in\n",
      "7: the\n",
      "8: with\n",
      "9: a\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:13:55.243355Z",
     "start_time": "2024-07-19T11:13:54.638140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_data = text_data[9]\n",
    "example_tokenized = vectorize_layer(example_data)\n",
    "print(example_tokenized.numpy())"
   ],
   "id": "4f5e474ffa0f00b9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 20:13:54.905456: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  24   14  571    1    8  306  340  186    4 1103  508   25  342  224\n",
      "  233  256    5  615   11  129   20  357    3  342   40  256    4  694\n",
      "    4   66    8  166    4   74    6    9   60   72    2  110    2   56\n",
      "   12  333    2   85  674   18   33    6    9   28   19    4   63  535\n",
      "   11  158    3  350  166   95    9  373  679  303   22   19    8  674\n",
      "    4   38    5  988    3   61    8   21    4   30    3  107   19    6\n",
      "  173  176 1324    4   55    5  134  105    2   44  721  571    8  278\n",
      "  233    4  253  285 1039    3  114  674   26   68    4  102   10  107\n",
      "    2   50  198   11  168  433  103  489    2  306    2  188    2   11\n",
      "   67  138   21    2    4   11   67  138   30    6    9   27   19    3\n",
      "   38    6  353    2 3758    2    4  145 1519  508    8 1347    2   31\n",
      "    2   11   53  138   30    2    4   11   67  138   21    6    9  286\n",
      "  184    5    9  418  608 3298  508    2   41  340  186    2   18  571\n",
      "    3    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:17:30.761039Z",
     "start_time": "2024-07-19T11:17:30.725568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 훈련 세트 만들기\n",
    "def prepare_inputs(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_ds = text_ds.map(prepare_inputs)"
   ],
   "id": "27012c2291d8987",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LSTM 만들기\n",
    "\n",
    "모델의 입력 = 정수 토큰의 시퀀스,  \n",
    "모델의 출력 = 10,000개 단어의 어휘 사전에서 시퀀스 다음에 나올 단어의 확률"
   ],
   "id": "9919b49adad6608f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:20:52.473997Z",
     "start_time": "2024-07-19T11:20:52.471635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 파라미터\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 200\n",
    "EMBEDDING_SIZE = 100\n",
    "N_UNITS = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25"
   ],
   "id": "2d58db03042a2e74",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T11:22:05.930364Z",
     "start_time": "2024-07-19T11:22:05.693478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = layers.Input(shape=(None,), dtype='int32')\n",
    "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_SIZE)(inputs)\n",
    "x = layers.LSTM(N_UNITS, return_sequences=True)(x)\n",
    "outputs = layers.Dense(VOCAB_SIZE, activation='softmax')(x)\n",
    "lstm = models.Model(inputs, outputs)\n",
    "lstm.summary()"
   ],
   "id": "34cfc68f051b21c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         1000000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 128)         117248    \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 10000)       1290000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2407248 (9.18 MB)\n",
      "Trainable params: 2407248 (9.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6f12f60fe788e3f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
